# From Closed-World Mastery to Meta-Game Exploration

This roadmap charts our path from mastering fixed-scenario optimization to building agents that can recognize, adapt to, and reason about uncertainty in the game itself.

Each phase builds on the previous, adding new capabilities while maintaining the engineering discipline and interpretability that make research tractable. Phases are designed to be independently publishable ‚Äî each delivers scientific value even if later phases aren't reached.

**Key principles**:
* Each phase has clear success criteria and deliverables
* Infrastructure improvements compound across phases
* Earlier phases derisk later ones through learning
* Community engagement starts early through open-source releases

---

## Phase 1 ‚Äî Closed-World Mastery

**Objective:** Efficiently discover strong policies for each fixed scenario.

**Timeline:** 2-3 months focused development

**Corresponds to**: Conceptual Milestone 1 (Closed-World Mastery)

### Technical Approach

#### Benchmarks
Lock in canonical scenarios covering diverse cooperation dynamics:

* **trivial_cooperation**: Pure coordination, no conflicts (baseline for max cooperation)
* **greedy_neighbor**: Self-interest vs. collective benefit (classic social dilemma)
* **sparse_heroics**: Rare high-risk, high-reward actions (exploration challenge)
* **rest_trap**: Temptation to defect early (temporal commitment problem)
* **chain_reaction**: Cascading consequences (long-term planning)
* **deceptive_calm**: Hidden risks (information asymmetry)
* **early_containment**: Prevention vs. reaction (timing challenge)
* **mixed_motivation**: Heterogeneous preferences (diverse objective challenge)
* **overcrowding**: Resource scarcity (tragedy of the commons)

Each scenario includes:
* Formal specification (reward function, dynamics, state space)
* Theoretical analysis (Nash equilibria when tractable)
* Baseline agents (heuristics, random, always-work/rest)

#### PPO Track

**Goal**: State-of-the-art single-scenario training

* **Vectorized environments**: Rust-backed parallel environments for 100-1000x speedup
* **Architecture**: Small MLPs (2-3 layers, 64-128 units) for interpretability
* **Hyperparameter optimization**: Optuna-based tuning per scenario
* **Sample efficiency tracking**: Episodes-to-convergence metrics
* **Automatic evaluation**: Regular checkpoint evaluation against baselines
* **Logging**: SQLite for structured metrics, JSON for configurations
* **Reproducibility**: Fixed seeds, documented environments, containerized training

**Success criteria**:
* Reach >90% of theoretical optimum (where known)
* Training converges in <1M timesteps (prefer <500K)
* Reproducible across 5 random seeds (variance <5% final performance)
* Training time <1 hour on consumer GPU (RTX 3080 or similar)

#### Genetic Algorithm Track

**Goal**: Complement PPO with evolutionary approach

* **Representation**: Parameterized heuristic agents (clear interpretability)
* **Selection**: Tournament selection (preserves diversity better than rank selection)
* **Mutation**: Gaussian perturbation of parameters
* **Niching**: Fitness sharing or crowding to maintain population diversity
* **Fitness evaluation**: Rust core for 100x speedup over Python
* **Parallel evaluation**: Distributed fitness computation

**Success criteria**:
* Match or exceed best hand-tuned heuristics
* Interpretable strategies (can explain behavior in terms of parameters)
* Faster iteration than PPO for initial exploration
* Discover strategies that differ qualitatively from PPO solutions

#### Nash Equilibrium Analysis

**Goal**: Theoretical diagnostic, not primary optimizer

* **Tractable scenarios**: Compute Nash equilibria for small state/action spaces
* **Approximations**: Use iterative methods for larger spaces
* **Comparison**: Measure empirical agents' distance from equilibrium
* **Insight**: Identify when learning fails to reach equilibrium (and why)

**Success criteria**:
* Complete analysis for ‚â•3 scenarios
* Clear visualization of equilibrium strategies
* Measured gap between empirical and theoretical performance
* Documented cases where Nash fails to predict learned behavior

#### Infrastructure

* **Leaderboard**: Web dashboard comparing all methods
* **Continuous evaluation**: Nightly runs on benchmark suite
* **Public results**: JSON files in repository for reproducibility
* **Documentation**: Training guides, hyperparameter settings, troubleshooting

### Success Criteria

**Quantitative**:
* PPO agents reach performance targets on all canonical scenarios
* Training time and sample efficiency meet targets
* Reproducibility across random seeds achieved
* GA discovers interpretable, competitive strategies
* Nash analysis completed for tractable scenarios

**Qualitative**:
* Clear documentation enables external reproduction
* Leaderboard provides intuitive performance comparison
* Infrastructure enables rapid iteration on new scenarios
* Results publishable as benchmark paper

### Deliverables

1. **Paper**: "Bucket Brigade: A Benchmark for Multi-Agent Cooperation Under Deception"
   * Scenario specifications
   * Baseline results (PPO, GA, heuristics)
   * Nash analysis where tractable
   * Open-source implementation

2. **Software Release**:
   * pip-installable Python package
   * Rust library for high-performance environments
   * Training scripts with sensible defaults
   * Evaluation and visualization tools

3. **Public Leaderboard**:
   * Web interface for results
   * Submission process for external researchers
   * Downloadable benchmark data

### Risks and Mitigation

**Risk**: Training doesn't converge reliably
* *Mitigation*: Start with simplest scenarios, tune hyperparameters systematically, consult MARL literature

**Risk**: Rust implementation is buggy, doesn't match Python semantics
* *Mitigation*: Extensive unit tests, cross-validation against Python, property-based testing

**Risk**: Scenarios too easy or too hard
* *Mitigation*: Iterative design, pilot testing, theoretical analysis of difficulty

**Risk**: External interest is low
* *Mitigation*: Focus on intrinsic scientific value, publish regardless, engage community early

### Phase 1 Completion Status

**Status**: ‚úÖ **EXCEEDED EXPECTATIONS** (as of 2025-11-05)

**Completed Work**:
* ‚úÖ **Evolution V4**: Discovered optimal strategies for all 9 scenarios (15,000 generations)
* ‚úÖ **Nash Equilibrium V2**: Game-theoretic validation of evolved strategies
* ‚úÖ **Phase 1.5**: Cross-scenario generalization analysis
* ‚ö†Ô∏è **PPO Track**: Deferred (not necessary - see Phase 1.5 findings)

**Major Discovery** (Phase 1.5):
All 9 evolved agents converged to **EXACTLY the same strategy** - a Universal Nash Equilibrium that achieves perfect generalization across all tested scenarios.

**Key Findings**:
* Genome identity: L2 distance = 0.0 (identical to 10+ decimal places)
* Transfer efficiency: 100% across all 9√ó9 = 81 scenario pairs
* No specialist vs generalist trade-off exists
* Universal "lazy free-rider" strategy is optimal across parameter ranges:
  - Œ≤ (fire spread): 0.10 to 0.30
  - c (work cost): 0.30 to 1.00

**Implications for Roadmap**:
* ‚úÖ Closed-world mastery achieved beyond expectations
* ‚úÖ Generalization already solved (no multi-scenario training needed)
* üîÑ Phase 2 refocused on heterogeneity and universality boundaries
* üìä Results publishable as major finding

**Documentation**:
* [Phase 1.5 Plan](../experiments/generalization/PHASE_1.5_PLAN.md)
* [Generalization Results](../experiments/generalization/GENERALIZATION_RESULTS.md)
* [Nash V2 Results](../experiments/nash/V2_RESULTS.md)

### Phase Completion Gate

‚úÖ **COMPLETED** (Phase 1 + Phase 1.5)

Phase 1 achievements exceed original goals:
* ‚úÖ Strong policies discovered (universal equilibrium)
* ‚úÖ Nash analysis completed (all 9 scenarios)
* ‚úÖ Infrastructure validated (Rust-accelerated evaluation)
* ‚úÖ Perfect generalization demonstrated
* ‚ö†Ô∏è PPO deferred (universal strategy makes it lower priority)
* ‚ö†Ô∏è External validation pending (paper publication)

---

## Phase 1.5 ‚Äî Cross-Scenario Generalization Analysis ‚úÖ COMPLETED

**Objective:** Determine whether evolved strategies are specialists or generalists.

**Timeline:** 2 days (planned) ‚Üí 2 hours (actual)

**Status**: ‚úÖ **COMPLETED** (2025-11-05)

### Findings

**Hypothesis**: Agents would specialize for their training scenarios with trade-offs when tested cross-scenario.

**Result**: All agents are IDENTICAL and achieve perfect generalization.

### Technical Approach

**Evaluation Matrix**: 9 agents √ó 9 test scenarios = 81 evaluations
* RustPayoffEvaluator with 2000 simulations per evaluation
* Total: 162,000 Monte Carlo rollouts
* Runtime: ~12 minutes (local sequential execution)

**Analysis**:
* Genome comparison: L2 distance between all pairs
* Performance matrix: Cross-scenario payoffs
* Transfer efficiency: Performance relative to Nash equilibrium
* Strategic interpretation: Parameter analysis

### Major Discovery

**Universal Nash Equilibrium Exists**

All 9 independent evolutionary runs (different scenarios, different starting conditions, 15,000 generations each) converged to the **exact same strategy**:

| Parameter | Value | Interpretation |
|-----------|-------|----------------|
| honesty | 0.306 | Low dishonesty |
| work_tendency | 0.064 | **Very low** - minimal work |
| neighbor_help | 0.015 | **Very low** - self-interested |
| own_priority | 0.907 | **High** - focus on own house |
| risk_aversion | 0.948 | **Very high** - conservative |
| rest_bias | 1.000 | **Maximum** - always prefer rest |

**Strategic Profile**: "Lazy Free-Rider with High Risk Aversion"

### Implications

**Phase 2 Redesign Required**:
* Original goal: Train agents that generalize across scenarios
* Finding: **Already achieved** - universal strategy works everywhere
* New focus: Heterogeneity, adaptation, universality boundaries

**Scientific Impact**:
* Symmetric self-play discovers universal solutions
* Game-theoretic equilibria robust across parameter variations
* No specialist vs generalist trade-off in tested parameter ranges
* Evolution reliably finds unique Nash equilibrium

### Deliverables

* ‚úÖ Performance matrix (9√ó9 cross-scenario evaluations)
* ‚úÖ Comprehensive analysis document
* ‚úÖ Evaluation infrastructure (reusable scripts)
* ‚úÖ Genome comparison and statistical validation

**Documentation**: [experiments/generalization/GENERALIZATION_RESULTS.md](../experiments/generalization/GENERALIZATION_RESULTS.md)

---

## Phase 2 ‚Äî Beyond the Universal Equilibrium (REVISED)

**Objective:** Explore the boundaries and limitations of the universal equilibrium discovered in Phase 1.5.

**Timeline:** 2-3 months development + experiments

**Status**: üîÑ IN PROGRESS (Phase 2A, 2A.1, 2D, and Scale Testing complete)

**Completed Work**:
* ‚úÖ **Phase 2A**: Universality boundary testing (extreme parameter scenarios)
* ‚úÖ **Phase 2A.1**: Trivial cooperation investigation (p_spark boundary)
* ‚úÖ **Phase 2D**: Mechanism design for cooperation (parameter-based approaches)
* ‚úÖ **Scale Testing**: Population-size invariance (N=4,6,8,10)

**Motivation**: Phase 1.5 found that a single strategy is optimal across all tested scenarios (Œ≤: 0.10-0.30, c: 0.30-1.00). Phase 2 investigates:
* Where does universality break down? (extreme parameters)
* Can we force heterogeneity? (different agent roles)
* How robust is the equilibrium? (off-equilibrium opponents)
* Can we design scenarios requiring cooperation? (mechanism design)

**Corresponds to**: New research direction (universality boundaries, not originally planned)

**Depends on**: Phase 1.5 complete ‚úÖ

### Technical Approach

Phase 2 investigates four complementary research directions:

#### 2A. Universality Boundary Testing ‚úÖ COMPLETED

**Goal**: Map the parameter space where universal equilibrium breaks down.

**Status**: ‚úÖ Completed 2025-11-05

**Key Finding**: Universal strategy is **more robust than expected** - performs better on extreme scenarios than baseline scenarios!

**Results**:
* **Tested Parameter Ranges**:
  * Œ≤ ‚àà [0.02, 0.75] (37.5√ó range)
  * c ‚àà [0.05, 5.00] (100√ó range)
* **Performance**:
  * Extreme scenarios mean: **63.78** payoff
  * Baseline scenarios mean: **51.63** payoff
  * "Degradation": **-23.5%** (actually improvement!)
* **Convergence**: 6/9 extreme scenarios achieved identical payoff (65.14), suggesting robust equilibrium
* **Exception**: Only `trivial_cooperation` (Œ∫=0.90, p_spark=0.0) shows poor performance (26.50)
  * Hypothesis: Universal strategy optimized for persistent threats, over-cooperates in trivial scenarios

**Hypotheses Results**:
* ‚ùå H1 (fails when Œ≤ > 0.40): False - strategy works well even at Œ≤=0.75 (wildfire)
* ‚ùå H2 (fails when c < 0.10): False - strategy achieves 65.14 at c=0.05 (free work)
* ‚è≥ H3 (scales to larger teams): Not yet tested

**Implications**:
* Universality extends far beyond training distribution
* No need to evolve specialized strategies for extreme parameters
* Strategy may have bounded applicability to transient/trivial threat scenarios

**Documentation**:
* [Phase 2A Analysis](../experiments/boundary_testing/PHASE_2A_ANALYSIS.md)
* Results: `experiments/boundary_testing/universal_strategy_test.json`

**Follow-up Work**:
* ‚úÖ **2A.1**: Investigated trivial cooperation anomaly - identified p_spark as critical boundary
* ‚úÖ **2A.2**: Team size variation (N=6, 8, 10) - completed as "Scale Testing"
* **2A.3**: Grid size variation - deferred to later

#### 2A.1. Trivial Cooperation Investigation ‚úÖ COMPLETED

**Goal**: Understand why trivial_cooperation scenario failed (payoff 26.50 vs 63.78 mean).

**Status**: ‚úÖ Completed 2025-11-05

**Key Discovery**: p_spark (ongoing fire probability) is the critical boundary parameter, not Œ∫ (extinguish rate).

**Approach**:
* Œ∫ sweep: Test Œ∫ ‚àà {0.60, 0.70, 0.80, 0.90} with p_spark=0
* p_spark sweep: Test p_spark ‚àà {0.00, 0.01, 0.02, 0.05} with Œ∫=0.90

**Results**:
* **Œ∫ sweep**: NO effect - all scenarios achieved identical payoff (26.50) when p_spark=0
* **p_spark sweep**: MASSIVE effect:
  * p_spark=0.00: 26.50 (poor - fires disappear)
  * p_spark=0.01: 59.28 (good - minimal persistence)
  * p_spark=0.02: 65.14 (best - optimal persistence) ‚òÖ
  * p_spark=0.03: 60-61 (good - moderate persistence)
  * p_spark=0.05: 48.53 (fair - high pressure)

**Key Finding**: Universal strategy optimized for **persistent threats** (p_spark > 0), not transient scenarios (p_spark=0).

**Goldilocks Zone**: Optimal p_spark ‚àà [0.02, 0.03] - enough persistence to maintain strategy effectiveness without overwhelming the system.

**Documentation**:
* [Phase 2A.1 Analysis](../experiments/boundary_testing/PHASE_2A1_ANALYSIS.md)
* Results: `experiments/boundary_testing/trivial_cooperation_analysis.json`

#### 2A.2. Scale Testing (Population-Size Invariance) ‚úÖ COMPLETED

**Goal**: Test if N=4 universal strategy scales to larger populations (N=6, 8, 10).

**Status**: ‚úÖ Completed 2025-11-05

**Remarkable Finding**: **PERFECT SCALING** - 0.00% degradation across all population sizes.

**Approach**:
* Test universal strategy on N ‚àà {4, 6, 8, 10}
* Test scenarios: chain_reaction, sparse_heroics, crisis_cheap, easy_spark_02
* Python-only evaluation (500 simulations) for compatibility

**Results**:
* All scenarios: **Identical payoffs** to 2+ decimal places across all N
* Mean degradation: **0.00%** for N=6, N=8, N=10
* Every single test: 0.00% degradation

**Key Finding**: Population-size invariance demonstrates this is a **dominant strategy equilibrium**, not just Nash equilibrium:
* Free-riding optimal regardless of population size
* Regardless of what others do
* Independent of beliefs or coordination

**Implications**:
* No need to evolve separate strategies for different N
* Same equilibrium applies to 4, 6, 8, 10... or larger populations
* Free-riding problem doesn't get better OR worse with group size

**Documentation**:
* [Scale Testing Analysis](../experiments/scale_testing/SCALE_TESTING_ANALYSIS.md)
* Results: `experiments/scale_testing/quick_results.json`

#### 2D. Mechanism Design for Cooperation ‚úÖ COMPLETED

**Goal**: Design scenarios where parameter variations can break free-riding equilibrium and induce cooperation.

**Status**: ‚úÖ Completed 2025-11-05

**Key Finding**: Parameter-based mechanism design **CANNOT** induce cooperation - free-riding equilibrium is extremely robust.

**Tested Mechanisms**:
1. **nearly_free_work** (c=0.01): Payoff 65.14 - good but no cooperation
2. **front_loaded_crisis** (high œÅ_ignite, p_spark=0): Payoff 24.50 - very poor (transient threat)
3. **sustained_pressure** (p_spark=0.10): Payoff 34.37 - poor (overwhelming pressure)
4. **high_stakes** (A=500, L=500): Payoff 60.91 - okay but no cooperation

**Result**: NO scenario induced cooperation. work_tendency remained at 0.064 (6.4%) across all mechanisms.

**Why Parameter-Based Approaches Fail**:
* Free-riding remains dominant under nearly-free work
* High stakes don't change incentive structure
* Crisis conditions don't require cooperation in current mechanics
* Sustained pressure overwhelms without changing equilibrium

**Would Need** (fundamental mechanic changes):
* Coordination bonuses (e.g., quadratic rewards: k¬∑(num_workers)¬≤)
* Information asymmetry (working reveals information)
* Punishment mechanisms (guilt, reputation systems)
* Temporal dependencies (early work prevents later catastrophe)

**Conclusion**: Within current game mechanics, cooperation cannot be induced through parameter variations alone.

**Documentation**:
* [Phase 2D Analysis](../experiments/mechanism_design/PHASE_2D_ANALYSIS.md)
* Results: `experiments/mechanism_design/results.json`

#### 2B. Heterogeneous Team Equilibria

**Goal**: Force agent diversity and study mixed-strategy equilibria.

**Approach**:
* **Role constraints**: Designate agents as "firefighter" or "coordinator"
  * Firefighters: Forced high work_tendency (>0.5)
  * Coordinators: Can choose any strategy
* **Asymmetric payoffs**: Different agents get different rewards
  * Owner: Higher reward for own house survival
  * Neighbor: Higher reward for helping others
* **Capability differences**: Some agents more/less effective at firefighting
  * Expert: Œ∫_effective = 2√ó normal
  * Novice: Œ∫_effective = 0.5√ó normal

**Research Questions**:
* Does the universal strategy still emerge for unconstrained agents?
* What equilibria arise with 50/50 firefighter/coordinator mix?
* Do asymmetric payoffs create specialized strategies?

**Baseline**: Compare heterogeneous teams to homogeneous universal agents

**Deliverable**: Taxonomy of equilibria under heterogeneity constraints

#### 2C. Adaptive Opponents and Robustness

**Goal**: Test universal strategy robustness against non-equilibrium opponents.

**Approach**:
* **Always-work opponent**: Agent that works 100% of the time
  * Tests if universal strategy exploits over-workers
* **Random opponent**: Uniformly random action selection
  * Tests baseline robustness
* **Adversarial opponent**: Trained to minimize universal strategy's payoff
  * Use PPO with negative reward = -payoff_universal
  * Tests worst-case exploitability
* **Evolving opponent**: Online learning that adapts during episode
  * Tests stability to opponent adaptation

**Research Questions**:
* Does universal strategy remain optimal against off-equilibrium opponents?
* Can adversarial training find exploits?
* What is the "price of universality" (suboptimality vs best response)?

**Metrics**:
* Payoff vs each opponent type
* Nash equilibrium when opponents use different strategies
* Regret: difference from best response to opponent

**Deliverable**: Robustness analysis and identification of exploitability

#### 2D. Mechanism Design for Cooperation

**Goal**: Design scenarios where free-riding equilibrium is suboptimal.

**Approach**:
* **Explicit coordination bonuses**:
  * Add reward for synchronized actions: R_coord = k¬∑(num_agents_working)¬≤
  * Tests if quadratic rewards incentivize cooperation
* **Punishment mechanisms**:
  * Penalty for resting when others work: R_guilt = -g¬∑(others_working)¬∑(self_resting)
  * Tests if guilt mechanisms break free-riding
* **Information asymmetry**:
  * Only working agents observe true fire state
  * Resting agents see noisy/delayed observations
  * Tests if information value incentivizes work
* **Temporal dependencies**:
  * Early work prevents later catastrophes
  * Late work is ineffective
  * Tests if foresight breaks lazy equilibrium

**Research Questions**:
* What mechanism parameters shift equilibrium from free-riding to cooperation?
* Can we design a scenario where work_tendency > 0.5 is optimal?
* What is the minimal intervention to achieve cooperation?

**Success Metric**: Identify at least one mechanism that produces work_tendency > 0.5

**Deliverable**: Design patterns for cooperation-inducing scenarios

### Success Criteria

**Phase 2A (Universality Boundaries)**:
* Map parameter space for Œ≤ ‚àà [0.01, 0.50], c ‚àà [0.05, 5.00]
* Identify at least one parameter regime where universal strategy fails
* Measure performance degradation outside tested ranges
* Document parameter discontinuities and phase transitions

**Phase 2B (Heterogeneous Teams)**:
* Train agents under 3+ role constraint configurations
* Document equilibria with mixed firefighter/coordinator teams
* Measure performance vs homogeneous universal baseline
* Identify conditions where specialization improves team performance

**Phase 2C (Adaptive Opponents)**:
* Test universal strategy vs 4+ opponent types
* Measure exploitability (payoff loss vs best response)
* Identify any opponent that significantly exploits universal strategy
* Document robustness bounds

**Phase 2D (Mechanism Design)**:
* Design 3+ mechanism interventions
* Achieve work_tendency > 0.5 in at least one designed scenario
* Measure cooperation levels (working frequency) under different mechanisms
* Identify minimal mechanism for cooperation

**Qualitative**:
* Clear understanding of universality limitations
* Documented design patterns for breaking free-riding
* Actionable insights for mechanism design
* Reusable methodology for boundary testing

### Deliverables

1. **Paper**: "The Limits of Universality: Boundary Conditions for Nash Equilibria in Symmetric Multi-Agent Games"
   * Phase 1.5 universal equilibrium discovery
   * Parameter space mapping (universality boundaries)
   * Heterogeneous equilibria analysis
   * Mechanism design for cooperation
   * Open questions: scalability, asymmetric games

2. **Extended Scenario Suite**:
   * Extreme parameter scenarios (Œ≤ ‚àà [0.01, 0.50], c ‚àà [0.05, 5.00])
   * Heterogeneous team configurations
   * Mechanism-augmented scenarios (coordination bonuses, etc.)
   * Baseline agents (universal strategy, specialists, adversarial)

3. **Analysis Tools**:
   * Parameter space visualization
   * Equilibrium taxonomy
   * Exploitability measurement
   * Mechanism effectiveness metrics

### Risks and Mitigation

**Risk**: Universal strategy remains optimal even at extreme parameters
* *Mitigation*: Test very wide parameter ranges, theoretical analysis of equilibrium conditions, consider fundamentally different game structures

**Risk**: Heterogeneous constraints don't produce interesting equilibria
* *Mitigation*: Try multiple constraint types, vary constraint strength, analyze failure modes

**Risk**: Universal strategy is unexploitable (perfectly robust)
* *Mitigation*: Try stronger adversarial training methods, test computational limits of exploitation search

**Risk**: No mechanism successfully induces cooperation
* *Mitigation*: Theoretical analysis of cooperation conditions, consult mechanism design literature, try extreme interventions

**Risk**: Findings are specific to Bucket Brigade, don't generalize
* *Mitigation*: Test on other symmetric game families, connect to theoretical game theory literature, identify common structures

### Phase Completion Gate

Proceed to Phase 3 when:
* ‚úÖ At least 2 of 4 research directions (2A-2D) completed
* ‚úÖ Universality boundaries documented (parameter space map)
* ‚úÖ At least one mechanism induces cooperation (work_tendency > 0.5)
* ‚úÖ Robustness analysis complete (universal strategy vs adversaries)
* ‚úÖ Results publishable (paper draft complete)
* ‚ö†Ô∏è Optional: All 4 directions completed (ideal but not required)

---

## Phase 3 ‚Äî Population-Level Resilience

**Objective:** Model cultural and evolutionary adaptation.

**Timeline:** 4-6 months (more experimental, exploratory phase)

**Corresponds to**: Conceptual Milestone 4 (Humility and Robustness) + exploration of population dynamics

**Depends on**: Phase 2 complete (need scenario-switching infrastructure)

### Technical Approach

#### Population Structure

Move from individual agents to evolving populations:

* **Population size**: 50-200 agents (trade-off between diversity and computational cost)
* **Heritable parameters**: Reward-weight vectors (e.g., w_self, w_group, w_fairness)
* **Initial diversity**: Randomized parameters or seeded from Phase 1/2 agents
* **Generational structure**: Discrete generations vs. continuous replacement

**Representation options**:
* **Parameterized heuristics**: Interpretable, fast evaluation, limited expressiveness
* **Neural networks**: More flexible, harder to interpret, expensive evaluation
* **Hybrid**: Neural backbone + interpretable reward-weight parameters

#### Selection and Reproduction

Evolutionary dynamics:

* **Fitness evaluation**: Episodic payoff in current scenario distribution
* **Selection mechanisms**:
  * Tournament selection (maintains diversity)
  * Fitness-proportional (stronger selection pressure)
  * Lexicase selection (specialists excel in different contexts)

* **Reproduction**:
  * **Mutation**: Gaussian noise on reward weights (œÉ = tunable parameter)
  * **Crossover**: Combine parameters from two parents (optional)
  * **Cloning**: Top performers replicate with small mutations

* **Elitism**: Preserve top N% unchanged (prevents catastrophic loss)

#### Catastrophic Perturbations

Simulate environmental shocks:

* **Types of catastrophes**:
  * **Scenario shift**: Switch to entirely new scenario
  * **Reward reweighting**: Change relative importance of objectives
  * **Population culling**: Remove fraction of population randomly
  * **Rule changes**: Modify environment dynamics

* **Timing**:
  * **Periodic**: Every N generations (predictable)
  * **Stochastic**: Random intervals (unpredictable)
  * **Threshold-triggered**: When diversity drops below threshold

* **Severity**: Controllable magnitude of change

#### Metrics for Resilience

Quantify population-level properties:

* **Diversity metrics**:
  * Shannon entropy of reward-weight distributions
  * Pairwise behavioral distance (strategy space diversity)
  * Phenotypic diversity (how many distinct behaviors?)
  * Genotypic diversity (parameter space coverage)

* **Performance metrics**:
  * Mean fitness (population average)
  * Variance of fitness (spread of performance)
  * Minimum fitness (worst-case robustness)
  * Pareto front (trade-offs captured)

* **Resilience metrics**:
  * **Recovery time**: Generations to restore pre-shock performance
  * **Performance drop**: Immediate impact of perturbation
  * **Adaptation rate**: Slope of fitness improvement post-shock
  * **Diversity correlation**: Does pre-shock diversity predict recovery?

#### Experimental Design

**Core hypothesis**: Diverse populations recover faster from environmental shocks

**Experiments**:

1. **Baseline**: No diversity maintenance (pure selection)
   * Expect: Fast optimization, low diversity, slow recovery

2. **Niching**: Explicitly maintain diversity through fitness sharing
   * Expect: Slower optimization, high diversity, fast recovery

3. **Catastrophe frequency**: Vary shock rate
   * Hypothesis: More frequent shocks favor diversity maintenance

4. **Catastrophe severity**: Vary magnitude of change
   * Hypothesis: Larger shocks benefit more from diversity

5. **Specialist vs. generalist**:
   * Do specialists emerge naturally?
   * Can specialists + generalists coexist stably?

### Success Criteria

**Quantitative**:
* Measurable correlation between pre-shock diversity and post-shock recovery (r > 0.5)
* Niching strategies outperform pure selection under catastrophes
* Recovery time <50% of initial convergence time
* Stable coexistence of distinct behavioral strategies (>2 clusters persisting >100 generations)

**Qualitative**:
* Clear visualization of diversity dynamics over time
* Interpretable specialization patterns (can explain what specialists do)
* Documented regime shifts (e.g., cooperation ‚Üí competition ‚Üí mixed)
* Connection to theoretical predictions from evolutionary biology

### Deliverables

1. **Paper**: "Diversity and Resilience in Multi-Agent Populations Under Environmental Shocks"
   * Empirical demonstration of diversity-resilience correlation
   * Comparison of selection mechanisms
   * Analysis of specialist-generalist dynamics
   * Connection to biological and cultural evolution literature

2. **Evolutionary Simulator**:
   * Population evolution framework
   * Configurable selection, mutation, catastrophe parameters
   * Rich metrics and visualization
   * Reproducible experiment configurations

3. **Case Studies**:
   * Documented examples of emergent specialization
   * Failure modes (when diversity doesn't help)
   * Videos/animations of population evolution

### Risks and Mitigation

**Risk**: Populations don't maintain diversity (converge too quickly)
* *Mitigation*: Tune selection pressure, implement niching, increase population size

**Risk**: Diversity-resilience correlation is weak or absent
* *Mitigation*: This is scientific result too; document null results, explore why

**Risk**: Computational cost prohibitive (large populations, long runs)
* *Mitigation*: Use parameterized agents not neural networks, leverage Rust speedup, cloud compute

**Risk**: Results not reproducible (stochastic, long runs, sensitive to initialization)
* *Mitigation*: Many random seeds, document variance, sensitivity analysis

**Risk**: Hard to interpret what "diversity" means or why it helps
* *Mitigation*: Multiple diversity metrics, ablation studies, manual analysis of evolved strategies

### Phase Completion Gate

Proceed to Phase 4 when:
* ‚úÖ Clear quantitative evidence for (or against) diversity-resilience hypothesis
* ‚úÖ Paper drafted with novel insights about population dynamics
* ‚úÖ Simulator released and documented
* ‚úÖ Connection to open-world robustness goals is clear

---

## Phase 4 ‚Äî Reflective and Norm-Forming Agents

**Objective:** Explore meta-ethics and alignment in miniature.

**Timeline:** 6-12 months (highly exploratory, research frontier)

**Corresponds to**: Conceptual Milestone 5 (Reflective Agents)

**Depends on**: Phases 2 and 3 complete (need inference and population dynamics)

**Note**: This is the most speculative phase. Success here would be groundbreaking; partial progress still valuable.

### Technical Approach

#### Reward-Rule Modification

Allow agents limited meta-reasoning about rewards:

* **Representation**: Reward function as explicit parameters
  * R(s,a) = w‚ÇÅ¬∑f‚ÇÅ(s,a) + w‚ÇÇ¬∑f‚ÇÇ(s,a) + ... + w‚Çô¬∑f‚Çô(s,a)
  * Features f·µ¢ are fixed (e.g., "house saved," "team coordination")
  * Weights w·µ¢ are mutable

* **Modification mechanisms**:
  * **Proposal**: Agent can suggest weight changes
  * **Voting**: Population votes on proposals (majority or consensus)
  * **Adoption**: Successful proposals update reward for proposer and/or group
  * **Inheritance**: Offspring inherit reward weights from parents

* **Constraints**: Prevent degenerate solutions
  * Bounded weight magnitudes
  * Rate limits on modifications
  * Cost to proposing changes (prevents spam)

#### Meta-Rule Dynamics

Track evolution of reward structures:

* **Cultural transmission**:
  * Horizontal: Agents observe and copy successful peers' reward weights
  * Vertical: Offspring inherit parents' weights with mutation
  * Oblique: Learn from best-performing agents regardless of lineage

* **Norm formation**:
  * Does population converge to shared reward structure?
  * Do stable polymorphisms emerge (multiple coexisting norms)?
  * What conditions favor cooperation vs. defection norms?

* **Regime analysis**:
  * Map parameter space of stable attractors
  * Identify phase transitions (cooperation ‚Üí defection)
  * Measure stability of equilibria

#### Experimental Questions

**Core investigations**:

1. **Emergent cooperation**: Can populations discover and maintain pro-social reward weights?
   * Start with selfish agents; do cooperative norms evolve?
   * What selection pressures favor group-beneficial rewards?

2. **Norm stability**: Once formed, do norms persist or oscillate?
   * Under what conditions do norms collapse?
   * Can counter-norms (defection) invade cooperative populations?

3. **Value learning**: Can agents learn robust meta-preferences across scenarios?
   * Do populations discover scenario-invariant principles?
   * E.g., "fairness," "reciprocity," "harm avoidance"

4. **Alignment proxy**: Is this a meaningful model of value alignment?
   * Do agents that question rewards behave more safely?
   * Does meta-reasoning improve robustness to reward misspecification?

#### Measurement and Analysis

**Metrics**:

* **Reward diversity**: Variance in population weight vectors
* **Norm consensus**: Clustering coefficient of weight space
* **Stability**: Persistence of dominant norms over time
* **Performance**: Fitness under current and held-out scenarios
* **Robustness**: Sensitivity to reward perturbations

**Qualitative analysis**:
* Manual inspection of evolved reward structures
* Case studies of successful norm formation
* Failure mode documentation
* Interpretable explanations of meta-reasoning

### Success Criteria

**Quantitative** (aspirational):
* Populations converge to shared reward structures (>70% weight agreement)
* Emerged norms are stable for >500 generations
* Agents with learned meta-rewards outperform fixed-reward agents on held-out distributions
* Observable correlation between meta-reasoning and robustness

**Qualitative** (more achievable):
* Documented examples of emergent norm formation
* Clear cases where meta-reasoning helps or hurts
* Connection to theoretical models from game theory, economics, or philosophy
* Framework applicable to other alignment research

**Minimal success** (still valuable):
* Working prototype for reward modification
* Quantitative characterization of when/how norms form
* Negative results with clear explanation
* Open questions and future directions for the field

### Deliverables

1. **Paper** (speculative): "Toward Reflective Agents: Learning What to Value in Multi-Agent Environments"
   * Novel paradigm for studying value learning
   * Empirical results on norm formation
   * Connection to AI alignment challenges
   * Honest assessment of limitations and open questions

2. **Prototype Framework**:
   * Reward-modification environment
   * Cultural transmission mechanisms
   * Analysis tools for norm dynamics
   * Reproducible experiment configurations

3. **Position Paper** (alternative): "Why Reward Functions Are Not Enough: A Case for Meta-Reasoning in MARL"
   * Philosophical grounding
   * Connection to alignment literature
   * Proposal for future research directions

### Risks and Mitigation

**Risk**: Too hard, no interesting results
* *Mitigation*: Start simple (discrete reward options, small parameter space), celebrate null results, pivot to more tractable questions

**Risk**: Agents exploit meta-reasoning (degenerate solutions)
* *Mitigation*: Careful constraint design, diverse evaluation scenarios, human oversight

**Risk**: Results don't generalize beyond Bucket Brigade
* *Mitigation*: Document scope carefully, connect to theoretical literature, propose extensions

**Risk**: Interpretation is ambiguous (is this "reflection" or just learning?)
* *Mitigation*: Operationalize terms carefully, compare to baselines without meta-reasoning, emphasize empirical contributions over philosophical claims

**Risk**: Community is skeptical (too speculative)
* *Mitigation*: Ground in prior work, emphasize concrete contributions, submit to venues open to exploratory work

### Phase Completion Gate

Proceed to Phase 5 when:
* ‚úÖ Some positive results (or highly informative negative results)
* ‚úÖ Working prototype released for community experimentation
* ‚úÖ Paper submitted or published (even if controversial)
* ‚úÖ Clear sense of what next steps should be (either continuing this direction or pivoting)

---

## Phase 5 ‚Äî Infrastructure & Dissemination

**Objective:** Make the platform accessible, maintainable, and impactful for the broader research community.

**Timeline:** Ongoing throughout all phases, with focused effort toward end

**Note**: This is not strictly sequential ‚Äî infrastructure work happens in parallel with research. Listed as final phase to emphasize its importance for long-term impact.

### Technical Infrastructure

#### Continuous Benchmarking

**Goal**: Automated evaluation on modest compute budgets

* **Benchmark suite**: Standardized evaluation on canonical scenarios
* **Continuous integration**: Nightly runs on latest code
* **Performance tracking**: Time-series of key metrics (sample efficiency, wall-clock time, final performance)
* **Regression detection**: Alerts when performance drops
* **Hardware targets**: Consumer GPU (RTX 3080 class), cloud instance (T4/V100), CPU-only fallback

**Implementation**:
* GitHub Actions for CI/CD
* Results stored in structured format (SQLite or JSON)
* Historical comparison and visualization
* Automatic issue creation for regressions

#### Visualization Dashboard

**Goal**: Intuitive exploration of results without code

* **Leaderboard**: Compare all methods across scenarios
* **Learning curves**: Interactive plots of training dynamics
* **Diversity visualizations**: Population evolution animations
* **Belief dynamics**: Scenario inference over time
* **Scenario browser**: Explore and customize scenarios
* **Result comparison**: A/B comparison of training runs

**Technology stack**:
* Web-based (accessible without installation)
* Static site (minimal hosting costs)
* Pre-rendered results + client-side interaction
* Mobile-friendly

#### Reproducibility

**Goal**: External researchers can reproduce all results

* **Frozen environments**: Containerized training (Docker)
* **Exact dependencies**: Locked Python/Rust versions
* **Seeded randomness**: Documented random seeds
* **Config management**: All hyperparameters in version-controlled files
* **Data availability**: Training logs and checkpoints archived
* **Code documentation**: Inline comments, architecture docs, tutorials

**Verification**:
* Regular external reproduction attempts
* "Reproducibility bounty" for finding bugs
* Hall of fame for successful reproductions

### API and Tooling

#### Python API

**User-friendly interface for researchers**:

```python
from bucket_brigade import BucketBrigadeEnv, PPOTrainer

# Create environment
env = BucketBrigadeEnv(scenario="greedy_neighbor", num_agents=6)

# Train agent
trainer = PPOTrainer(env, config="configs/default_ppo.yaml")
trainer.train(total_timesteps=500_000)

# Evaluate
results = trainer.evaluate(num_episodes=100)
print(f"Mean reward: {results['mean_reward']:.2f}")
```

**Features**:
* Gym-compatible interface
* Automatic vectorization
* Sensible defaults with full configurability
* Rich logging and callbacks
* Integration with popular MARL frameworks

#### Rust API

**High-performance core for custom research**:

```rust
use bucket_brigade::{BucketBrigadeGame, Scenario};

let scenario = Scenario::greedy_neighbor();
let game = BucketBrigadeGame::new(scenario, 6);

// Vectorized batch evaluation
let observations = game.reset_batch(1000);
let actions = policy.act_batch(&observations);
let (rewards, dones) = game.step_batch(actions);
```

**Features**:
* Zero-copy operations
* Parallel episode execution
* Minimal dependencies
* C FFI for Python bindings
* WebAssembly support for browser demos

#### Notebooks and Tutorials

**Learning resources**:

* **Quickstart**: Train first agent in 5 minutes
* **Scenario design**: Create custom scenarios
* **Hyperparameter tuning**: Optuna integration guide
* **Advanced**: Custom architectures, multi-scenario training
* **Analysis**: Visualizing and interpreting results
* **Theory**: Nash equilibrium computation

**Formats**:
* Jupyter notebooks (interactive)
* Colab notebooks (zero-install)
* Markdown tutorials (static docs)
* Video walkthroughs (YouTube)

### Community Engagement

#### Publications

**Target venues**:
* **ICLR/NeurIPS/ICML**: Phase 1 benchmark paper, Phase 2 inference paper
* **AAMAS/IJCAI**: Multi-agent cooperation focus
* **CoRL**: If robotics applications emerge
* **AAAI/JAIR**: Theoretical analysis papers
* **PLOS Comp Bio**: Evolutionary dynamics papers
* **AI Safety venues**: Phase 4 alignment work

**Publication strategy**:
* Preprints on arXiv immediately
* Workshop papers for early feedback
* Open peer review when possible
* Registered reports for pre-commitment

#### Open Source

**Repository organization**:
* Clean README with quick examples
* Contribution guidelines
* Code of conduct
* Issue templates
* PR checklist
* Changelog

**Community features**:
* Discord or Slack for discussions
* Regular "office hours" for help
* Bounties for feature contributions
* Highlighted community projects

#### Workshops and Tutorials

**Events**:
* Conference workshops (MARL, AI Safety)
* Tutorial sessions at major venues
* University guest lectures
* Industry collaboration meetings

**Materials**:
* Slide decks (publicly available)
* Recorded presentations
* Hands-on coding sessions
* Benchmark competition

### Success Criteria

**Quantitative**:
* ‚â•100 GitHub stars (community interest)
* ‚â•10 external papers using Bucket Brigade
* ‚â•5 successful external reproductions
* ‚â•3 community-contributed features merged
* Package downloads >1000/month

**Qualitative**:
* Cited in alignment research discussions
* Used in university courses
* Featured in ML news/blogs
* Positive community feedback
* Growing contributor base

### Deliverables

1. **Production-Ready Platform**:
   * pip/cargo installable
   * Comprehensive documentation
   * Active maintenance and support
   * Regular releases with changelogs

2. **Public Dashboard**:
   * Live leaderboard
   * Interactive visualizations
   * Result browser and comparison
   * Mobile-friendly design

3. **Educational Materials**:
   * Tutorials and notebooks
   * Video walkthroughs
   * Example projects
   * Teaching resources for instructors

4. **Community Resources**:
   * Active communication channels
   * Contribution pathways
   * Recognition for contributors
   * Sustainable maintenance plan

### Long-Term Sustainability

**Funding**:
* Grant applications (NSF, foundations, industry)
* Institutional support
* Potential for commercialization (e.g., training platform as service)

**Maintenance**:
* Core team commitment (at least 2 years)
* Succession planning
* Documentation for handoff
* Community governance if project grows large

**Evolution**:
* Incorporate community feedback
* Extend to related domains
* Support emerging research directions
* Balance stability with innovation

---

## End State and Vision

After completing these phases, Bucket Brigade will be:

### A Scientific Platform

**For Multi-Agent RL Research**:
* Canonical benchmark for cooperation under deception
* Fast, reproducible evaluation infrastructure
* Rich scenario library covering diverse social dilemmas
* Theoretical baselines (Nash equilibria) for validation

**For AI Alignment Research**:
* Concrete testbed for reward misspecification
* Framework for studying value learning and meta-reasoning
* Demonstrations of epistemic humility and robustness
* Bridge between philosophical questions and empirical results

**For Evolutionary Biology and Social Science**:
* Quantitative models of cultural evolution
* Empirical tests of diversity-resilience hypotheses
* Laboratory for studying norm formation
* Computational social science playground

### A Community Resource

**Infrastructure**:
* Production-ready software (Python + Rust)
* Comprehensive documentation and tutorials
* Active maintenance and support
* Open development process

**Ecosystem**:
* Growing user base (academic + industry)
* Community contributions (scenarios, agents, analyses)
* Educational adoption (university courses)
* Industry partnerships (applied research)

**Impact**:
* Multiple publications and citations
* Influence on MARL and alignment research directions
* Practical applications in real-world multi-agent systems
* Training ground for next generation of researchers

### A Research Program

**Near-term (1-2 years)**:
* Establish benchmark credibility (Phase 1)
* Demonstrate scenario adaptation (Phase 2)
* Initial population dynamics studies (Phase 3)

**Mid-term (2-4 years)**:
* Publish major papers on game inference and population resilience
* Attract external collaborators and extensions
* Influence alignment research conversations

**Long-term (4+ years)**:
* Explore reflective agents and norm formation (Phase 4)
* Expand to related domains (communication, resource allocation)
* Scale to larger, more complex scenarios
* Contribute to safe deployment of multi-agent AI systems

### Success Metrics

**Technical Excellence**:
* ‚úÖ Faster training than comparable MARL benchmarks
* ‚úÖ Reproducible results across research groups
* ‚úÖ Clear interpretability of learned behaviors
* ‚úÖ Robust performance on held-out distributions

**Scientific Impact**:
* ‚úÖ Publications in top venues (ICLR, NeurIPS, AAMAS, etc.)
* ‚úÖ Citations from alignment and MARL communities
* ‚úÖ Novel empirical findings about cooperation and adaptation
* ‚úÖ Theoretical advances in understanding multi-agent learning

**Community Adoption**:
* ‚úÖ External research papers using Bucket Brigade
* ‚úÖ University courses incorporating the platform
* ‚úÖ Industry applications and partnerships
* ‚úÖ Growing open-source contributor base

**Philosophical Contribution**:
* ‚úÖ Concrete operationalization of "epistemic humility"
* ‚úÖ Empirical grounding for value learning theories
* ‚úÖ Bridge between optimization and reflection
* ‚úÖ Framework for thinking about AI alignment in multi-agent contexts

---

## Summary: The Journey from Optimization to Understanding

This roadmap charts an ambitious path:

**We start** with closed-world mastery ‚Äî becoming excellent at discovering optimal policies for fixed, well-specified problems. This establishes credibility, builds infrastructure, and validates our engineering.

**We bridge** to adaptive agents ‚Äî systems that recognize when the game has changed and update their beliefs and strategies accordingly. This introduces uncertainty-awareness and robustness.

**We explore** population-level dynamics ‚Äî how diversity buffers against catastrophic change, how specialization emerges, and how cultural transmission shapes behavioral repertoires.

**We reach toward** reflective intelligence ‚Äî agents that question their own objectives, reason about what they *should* value, and maintain humility about their models of the world.

**We deliver** a platform that serves the research community ‚Äî through open-source software, reproducible benchmarks, educational materials, and sustained engagement.

### Why This Matters

The path from closed to open worlds isn't just an incremental extension of current MARL research. It addresses fundamental questions:

* **Can we build AI systems that know they don't know?** Systems with genuine epistemic humility, not just overconfident predictions.

* **How do values emerge and stabilize in populations?** Understanding this in simple multi-agent environments might inform how to shape values in more complex AI systems.

* **What does it mean for an agent to "understand" a game?** Not just optimize within it, but recognize its structure, question its assumptions, and reason about alternatives.

* **Can optimization produce wisdom, or only competence?** And if the latter, what complementary capacities do we need to develop?

These are the questions that matter for deploying powerful AI systems safely. Bucket Brigade provides a concrete, interpretable environment to explore them rigorously.

### The Path Forward

We don't expect to solve AI alignment through this platform alone. But we can:

* **Demonstrate** that concepts like "game inference" and "epistemic humility" can be operationalized and measured
* **Validate** (or refute) hypotheses about diversity, adaptation, and value learning
* **Provide** tools that other researchers can build on
* **Train** the next generation of alignment researchers with concrete, tractable problems
* **Bridge** the gap between philosophical speculation and empirical validation

The journey from closed-world mastery to meta-game exploration is long. But each phase delivers independent value. Each milestone adds capability. Each publication advances the field.

We begin with the physics lab of multi-agent cooperation. We aim toward the first glimmers of reflective intelligence.

---

## See Also

* [Closed vs. Open World Background](background_closed_vs_open_world.md) ‚Äî Conceptual foundation and philosophical motivation
* [Vision and Future Directions](vision_future_directions.md) ‚Äî Long-term research questions and scientific payoff
* [Current Status](README.md) ‚Äî Implementation progress and active work
* [Game Mechanics](game_mechanics.md) ‚Äî Detailed rules and scenario specifications
* [Hyperparameter Tuning](HYPERPARAMETER_TUNING.md) ‚Äî Practical guide for Phase 1 optimization
