{
  "name": "early_containment",
  "description": "Early Containment - fires start aggressive but can be stopped early",
  "parameters": {
    "beta": 0.35,
    "kappa": 0.6,
    "A": 100.0,
    "L": 100.0,
    "c": 0.5,
    "rho_ignite": 0.3,
    "N_min": 12,
    "p_spark": 0.02,
    "N_spark": 12,
    "num_agents": 4
  },
  "research_config": {
    "num_heuristic_games": 100,
    "evolution_generations": 200,
    "evolution_population": 100,
    "nash_simulations": 1000,
    "nash_max_iterations": 50
  },
  "story": "Fires start aggressively with high spread rate (\u03b2=0.35) and many initial fires (30%), but moderate extinguish efficiency (\u03ba=0.6) means coordinated early action can contain them. Delaying response leads to cascading failures across the town.",
  "research_questions": [
    "How quickly must agents respond to prevent chain reactions?",
    "Is early coordination more valuable than distributed coverage?",
    "Can signaling help agents prioritize burning houses?",
    "What happens if agents delay to conserve energy for later nights?"
  ],
  "research_insights": [
    {
      "question": "What does Nash equilibrium predict about cooperation in this scenario?",
      "finding": "Nash equilibrium predicts complete free-riding (0% cooperation) with expected payoff of 24.3.",
      "evidence": [
        "Equilibrium type: mixed",
        "Strategy classification: Free Rider",
        "Expected payoff: 24.27",
        "Cooperation rate: 0%",
        "Convergence: 3 iterations in 1646.9s"
      ],
      "implication": "Individual rationality leads to collective failure. Without external coordination mechanisms, rational agents will defect even when cooperation benefits everyone."
    },
    {
      "question": "How effective was evolutionary optimization?",
      "finding": "Evolution improved from -3.0 to 0.2 payoff over 1001 generations (\u0394 3.3).",
      "evidence": [
        "Initial best fitness: -3.02",
        "Final best fitness: 0.24",
        "Total improvement: 3.26",
        "Generations: 1001",
        "Converged: False"
      ],
      "implication": "Limited improvement suggests either strong initial random strategies or a complex fitness landscape with many local optima."
    },
    {
      "question": "What does population diversity reveal about the fitness landscape?",
      "finding": "Population reached moderate diversity (0.289), balancing exploration and exploitation.",
      "evidence": [
        "Final diversity: 0.289",
        "Mid-run diversity: 0.252",
        "Best generation: 1000"
      ],
      "implication": "Evolution balanced convergence toward good strategies while maintaining variation for continued adaptation."
    },
    {
      "question": "How much performance is lost at Nash equilibrium?",
      "finding": "Nash equilibrium (24.3) falls 44.5 points short of optimal play (68.8) - a 184% efficiency loss.",
      "evidence": [
        "Nash payoff: 24.27",
        "Optimal payoff: 68.80",
        "Gap: 44.53",
        "Efficiency: 35.3%"
      ],
      "implication": "Significant coordination failure. Individual rationality produces collectively suboptimal outcomes, indicating need for external coordination mechanisms."
    },
    {
      "question": "Can evolution discover strategies superior to Nash equilibrium?",
      "finding": "Yes - evolved strategies achieve 68.8 payoff, outperforming Nash equilibrium (24.3) by 44.5 points.",
      "evidence": [
        "Evolved payoff: 68.80",
        "Nash payoff: 24.27",
        "honesty: 0.70 (Nash) vs 1.00 (Evolved)",
        "work_tendency: 0.20 (Nash) vs 0.67 (Evolved)"
      ],
      "implication": "Evolution can escape suboptimal equilibria by discovering innovative strategies that Nash analysis misses. Natural selection may find solutions that pure rationality cannot."
    },
    {
      "question": "Which agent parameters matter most for success in this scenario?",
      "finding": "The parameter 'risk_aversion' shows the largest difference between Nash and optimal strategies, suggesting it's critical for performance.",
      "evidence": [
        "Nash strategy risk_aversion: 0.00",
        "Best strategy risk_aversion: 1.00",
        "Difference: 1.00",
        "Also important: own_priority (\u0394 0.88)",
        "Also important: rest_bias (\u0394 0.57)"
      ],
      "implication": "Success in this scenario critically depends on optimizing risk_aversion. Strategies that neglect this parameter face significant performance penalties."
    }
  ],
  "method_insights": {
    "nash": [
      {
        "question": "What does Nash equilibrium predict about cooperation in this scenario?",
        "finding": "Nash equilibrium predicts complete free-riding (0% cooperation) with expected payoff of 24.3.",
        "evidence": [
          "Equilibrium type: mixed",
          "Strategy classification: Free Rider",
          "Expected payoff: 24.27",
          "Cooperation rate: 0%",
          "Convergence: 3 iterations in 1646.9s"
        ],
        "implication": "Individual rationality leads to collective failure. Without external coordination mechanisms, rational agents will defect even when cooperation benefits everyone."
      }
    ],
    "evolution": [
      {
        "question": "How effective was evolutionary optimization?",
        "finding": "Evolution improved from -3.0 to 0.2 payoff over 1001 generations (\u0394 3.3).",
        "evidence": [
          "Initial best fitness: -3.02",
          "Final best fitness: 0.24",
          "Total improvement: 3.26",
          "Generations: 1001",
          "Converged: False"
        ],
        "implication": "Limited improvement suggests either strong initial random strategies or a complex fitness landscape with many local optima."
      },
      {
        "question": "What does population diversity reveal about the fitness landscape?",
        "finding": "Population reached moderate diversity (0.289), balancing exploration and exploitation.",
        "evidence": [
          "Final diversity: 0.289",
          "Mid-run diversity: 0.252",
          "Best generation: 1000"
        ],
        "implication": "Evolution balanced convergence toward good strategies while maintaining variation for continued adaptation."
      }
    ],
    "comparative": [
      {
        "question": "How much performance is lost at Nash equilibrium?",
        "finding": "Nash equilibrium (24.3) falls 44.5 points short of optimal play (68.8) - a 184% efficiency loss.",
        "evidence": [
          "Nash payoff: 24.27",
          "Optimal payoff: 68.80",
          "Gap: 44.53",
          "Efficiency: 35.3%"
        ],
        "implication": "Significant coordination failure. Individual rationality produces collectively suboptimal outcomes, indicating need for external coordination mechanisms."
      },
      {
        "question": "Can evolution discover strategies superior to Nash equilibrium?",
        "finding": "Yes - evolved strategies achieve 68.8 payoff, outperforming Nash equilibrium (24.3) by 44.5 points.",
        "evidence": [
          "Evolved payoff: 68.80",
          "Nash payoff: 24.27",
          "honesty: 0.70 (Nash) vs 1.00 (Evolved)",
          "work_tendency: 0.20 (Nash) vs 0.67 (Evolved)"
        ],
        "implication": "Evolution can escape suboptimal equilibria by discovering innovative strategies that Nash analysis misses. Natural selection may find solutions that pure rationality cannot."
      },
      {
        "question": "Which agent parameters matter most for success in this scenario?",
        "finding": "The parameter 'risk_aversion' shows the largest difference between Nash and optimal strategies, suggesting it's critical for performance.",
        "evidence": [
          "Nash strategy risk_aversion: 0.00",
          "Best strategy risk_aversion: 1.00",
          "Difference: 1.00",
          "Also important: own_priority (\u0394 0.88)",
          "Also important: rest_bias (\u0394 0.57)"
        ],
        "implication": "Success in this scenario critically depends on optimizing risk_aversion. Strategies that neglect this parameter face significant performance penalties."
      }
    ]
  }
}